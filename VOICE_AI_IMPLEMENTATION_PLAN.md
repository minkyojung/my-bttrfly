# ğŸ¯ William ìŒì„± ëŒ€í™” AI - ìƒì„¸ ì‹¤í–‰ ê³„íš

## ğŸ“ í”„ë¡œì íŠ¸ ì•„í‚¤í…ì²˜

### ì‹œìŠ¤í…œ êµ¬ì¡°ë„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Frontend Layer                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Next.js App                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Chat UI      â”‚  â”‚ Voice UI     â”‚  â”‚ Dashboard    â”‚ â”‚
â”‚  â”‚ (ê¸°ì¡´)       â”‚  â”‚ (ì‹ ê·œ)       â”‚  â”‚ (ê¸°ì¡´)       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API Layer (Next.js)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  /api/chat (ê¸°ì¡´)       - í…ìŠ¤íŠ¸ RAG                    â”‚
â”‚  /api/voice/transcribe  - STT (ì‹ ê·œ)                    â”‚
â”‚  /api/voice/synthesize  - TTS (ì‹ ê·œ)                    â”‚
â”‚  /api/voice/chat        - í†µí•© API (ì‹ ê·œ)               â”‚
â”‚  /api/voice/streaming   - ìŠ¤íŠ¸ë¦¬ë° (Phase 2)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  External Services                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  OpenAI          - Whisper STT, GPT-4o-mini            â”‚
â”‚  ElevenLabs      - TTS, Voice Cloning                  â”‚
â”‚  Supabase        - Vector DB (ê¸°ì¡´)                     â”‚
â”‚  Cohere          - Reranking (ê¸°ì¡´)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ í”„ë¡œì íŠ¸ íŒŒì¼ êµ¬ì¡°

```
/Users/williamjung/conductor/my-bttrfly/.conductor/warsaw/
â”‚
â”œâ”€ app/
â”‚  â”œâ”€ api/
â”‚  â”‚  â”œâ”€ chat/
â”‚  â”‚  â”‚  â””â”€ route.ts                    (ê¸°ì¡´)
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ voice/                         (ì‹ ê·œ)
â”‚  â”‚     â”œâ”€ transcribe/
â”‚  â”‚     â”‚  â””â”€ route.ts                 [Phase 1] STT ì—”ë“œí¬ì¸íŠ¸
â”‚  â”‚     â”œâ”€ synthesize/
â”‚  â”‚     â”‚  â””â”€ route.ts                 [Phase 1] TTS ì—”ë“œí¬ì¸íŠ¸
â”‚  â”‚     â”œâ”€ chat/
â”‚  â”‚     â”‚  â””â”€ route.ts                 [Phase 1] í†µí•© ìŒì„± ì±„íŒ…
â”‚  â”‚     â””â”€ streaming/
â”‚  â”‚        â””â”€ route.ts                 [Phase 2] ìŠ¤íŠ¸ë¦¬ë° TTS
â”‚  â”‚
â”‚  â””â”€ voice/                            (ì‹ ê·œ)
â”‚     â””â”€ page.tsx                       [Phase 1] ìŒì„± ì±„íŒ… í˜ì´ì§€
â”‚
â”œâ”€ components/
â”‚  â”œâ”€ VoiceRecorder.tsx                 [Phase 1] ìŒì„± ë…¹ìŒ ì»´í¬ë„ŒíŠ¸
â”‚  â”œâ”€ VoicePlayer.tsx                   [Phase 1] ìŒì„± ì¬ìƒ ì»´í¬ë„ŒíŠ¸
â”‚  â”œâ”€ VoiceChatInterface.tsx            [Phase 1] í†µí•© ì¸í„°í˜ì´ìŠ¤
â”‚  â””â”€ WaveformVisualizer.tsx            [Phase 1] ìŒì„± ì‹œê°í™”
â”‚
â”œâ”€ lib/
â”‚  â”œâ”€ voice/                            (ì‹ ê·œ)
â”‚  â”‚  â”œâ”€ stt.ts                         [Phase 1] STT í´ë¼ì´ì–¸íŠ¸
â”‚  â”‚  â”œâ”€ tts.ts                         [Phase 1] TTS í´ë¼ì´ì–¸íŠ¸
â”‚  â”‚  â”œâ”€ audio-utils.ts                 [Phase 1] ì˜¤ë””ì˜¤ ìœ í‹¸ë¦¬í‹°
â”‚  â”‚  â””â”€ voice-session.ts               [Phase 2] ì„¸ì…˜ ê´€ë¦¬
â”‚  â”‚
â”‚  â””â”€ rag/                              (ê¸°ì¡´ ë¦¬íŒ©í† ë§)
â”‚     â”œâ”€ retrieval.ts                   ê¸°ì¡´ RAG ë¡œì§ ë¶„ë¦¬
â”‚     â””â”€ chat.ts                        ê¸°ì¡´ chat ë¡œì§ ë¶„ë¦¬
â”‚
â”œâ”€ types/
â”‚  â””â”€ voice.ts                          [Phase 1] ìŒì„± íƒ€ì… ì •ì˜
â”‚
â””â”€ .env.local
   â”œâ”€ OPENAI_API_KEY                    (ê¸°ì¡´)
   â”œâ”€ ELEVENLABS_API_KEY                (ì‹ ê·œ)
   â””â”€ WILLIAM_VOICE_ID                  (ì‹ ê·œ)
```

---

## ğŸ”§ Phase 1: ê¸°ë³¸ ìŒì„± ì±„íŒ… (ë¹„ë™ê¸°)

### Week 1, Day 1-2: í™˜ê²½ ì„¤ì • & ê¸°ì´ˆ êµ¬ì¡°

#### âœ… Task 1.1: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

**íŒŒì¼**: `.env.local`

```bash
# ê¸°ì¡´
NEXT_PUBLIC_SUPABASE_URL=...
SUPABASE_SERVICE_ROLE_KEY=...
OPENAI_API_KEY=...
COHERE_API_KEY=...

# ì‹ ê·œ ì¶”ê°€
ELEVENLABS_API_KEY=your_api_key_here
WILLIAM_VOICE_ID=your_voice_id_here
```

**ì‹¤í–‰ ë‹¨ê³„**:
1. ElevenLabs ê°€ì…: https://elevenlabs.io
2. API í‚¤ ë°œê¸‰: Settings â†’ API Keys
3. `.env.local`ì— ì¶”ê°€

---

#### âœ… Task 1.2: íŒ¨í‚¤ì§€ ì„¤ì¹˜

**íŒŒì¼**: `package.json`

```json
{
  "dependencies": {
    // ê¸°ì¡´...

    // ì‹ ê·œ ì¶”ê°€
    "elevenlabs": "^0.15.0",
    "formidable": "^3.5.1",
    "@types/formidable": "^3.4.5"
  }
}
```

**ì‹¤í–‰**:
```bash
npm install elevenlabs formidable
npm install -D @types/formidable
```

---

#### âœ… Task 1.3: íƒ€ì… ì •ì˜

**íŒŒì¼**: `types/voice.ts` (ì‹ ê·œ)

```typescript
/**
 * ìŒì„± ê´€ë ¨ íƒ€ì… ì •ì˜
 */

export interface TranscriptionRequest {
  audioFile: File;
  language?: string;
}

export interface TranscriptionResponse {
  text: string;
  duration: number;
  language: string;
}

export interface SynthesisRequest {
  text: string;
  voiceId?: string;
  modelId?: string;
}

export interface SynthesisResponse {
  audio: ArrayBuffer;
  contentType: string;
}

export interface VoiceChatRequest {
  audioFile: File;
  sessionId?: string;
  history?: Message[];
}

export interface VoiceChatResponse {
  transcript: string;
  response: string;
  audio: ArrayBuffer;
  sources?: Source[];
  sessionId: string;
}

export interface Message {
  role: 'user' | 'assistant' | 'system';
  content: string;
  timestamp?: Date;
}

export interface Source {
  id: string;
  title: string;
  content: string;
  url: string | null;
  similarity: number;
}

export interface VoiceSession {
  sessionId: string;
  userId?: string;
  history: Message[];
  createdAt: Date;
  lastActivityAt: Date;
}

// ì—ëŸ¬ íƒ€ì…
export class VoiceError extends Error {
  constructor(
    message: string,
    public code: string,
    public statusCode: number = 500
  ) {
    super(message);
    this.name = 'VoiceError';
  }
}
```

---

### Week 1, Day 3-4: ë°±ì—”ë“œ API êµ¬í˜„

#### âœ… Task 1.4: STT API

**íŒŒì¼**: `app/api/voice/transcribe/route.ts` (ì‹ ê·œ)

```typescript
/**
 * STT (Speech-to-Text) API
 * OpenAI Whisperë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
 */

import { NextRequest, NextResponse } from 'next/server';
import OpenAI from 'openai';
import { formidable } from 'formidable';
import { VoiceError } from '@/types/voice';
import fs from 'fs';

// Vercel Edge FunctionsëŠ” formidableì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ
// Node.js runtime ì‚¬ìš©
export const runtime = 'nodejs';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Constants
const MAX_FILE_SIZE = 25 * 1024 * 1024; // 25MB (Whisper limit)
const ALLOWED_FORMATS = ['audio/webm', 'audio/mp4', 'audio/mpeg', 'audio/wav'];

export async function POST(req: NextRequest) {
  try {
    // 1. FormDataì—ì„œ íŒŒì¼ ì¶”ì¶œ
    const formData = await req.formData();
    const audioFile = formData.get('audio') as File;

    if (!audioFile) {
      throw new VoiceError('ìŒì„± íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.', 'NO_AUDIO_FILE', 400);
    }

    // 2. íŒŒì¼ ê²€ì¦
    if (audioFile.size > MAX_FILE_SIZE) {
      throw new VoiceError(
        'íŒŒì¼ í¬ê¸°ëŠ” 25MBë¥¼ ì´ˆê³¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
        'FILE_TOO_LARGE',
        400
      );
    }

    if (!ALLOWED_FORMATS.includes(audioFile.type)) {
      throw new VoiceError(
        `ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. (${audioFile.type})`,
        'UNSUPPORTED_FORMAT',
        400
      );
    }

    // 3. Whisper API í˜¸ì¶œ
    const startTime = Date.now();

    const transcription = await openai.audio.transcriptions.create({
      file: audioFile,
      model: 'whisper-1',
      language: 'ko', // í•œêµ­ì–´ ìš°ì„ 
      response_format: 'verbose_json', // íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨
    });

    const duration = Date.now() - startTime;

    // 4. ì‘ë‹µ ë°˜í™˜
    return NextResponse.json({
      text: transcription.text,
      duration,
      language: transcription.language || 'ko',
      segments: transcription.segments, // ë¬¸ì¥ë³„ íƒ€ì„ìŠ¤íƒ¬í”„
    });

  } catch (error) {
    console.error('STT Error:', error);

    if (error instanceof VoiceError) {
      return NextResponse.json(
        { error: error.message, code: error.code },
        { status: error.statusCode }
      );
    }

    return NextResponse.json(
      { error: 'ìŒì„± ì¸ì‹ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.', code: 'STT_ERROR' },
      { status: 500 }
    );
  }
}
```

---

#### âœ… Task 1.5: TTS API

**íŒŒì¼**: `app/api/voice/synthesize/route.ts` (ì‹ ê·œ)

```typescript
/**
 * TTS (Text-to-Speech) API
 * ElevenLabsë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜
 */

import { NextRequest, NextResponse } from 'next/server';
import { ElevenLabsClient } from 'elevenlabs';
import { VoiceError } from '@/types/voice';

const elevenlabs = new ElevenLabsClient({
  apiKey: process.env.ELEVENLABS_API_KEY,
});

const WILLIAM_VOICE_ID = process.env.WILLIAM_VOICE_ID || '';
const MAX_TEXT_LENGTH = 5000; // ElevenLabs ì œí•œ

export async function POST(req: NextRequest) {
  try {
    const { text, voiceId, modelId } = await req.json();

    // 1. ì…ë ¥ ê²€ì¦
    if (!text || !text.trim()) {
      throw new VoiceError('í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.', 'NO_TEXT', 400);
    }

    if (text.length > MAX_TEXT_LENGTH) {
      throw new VoiceError(
        `í…ìŠ¤íŠ¸ëŠ” ${MAX_TEXT_LENGTH}ìë¥¼ ì´ˆê³¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.`,
        'TEXT_TOO_LONG',
        400
      );
    }

    if (!WILLIAM_VOICE_ID) {
      throw new VoiceError(
        'William ìŒì„±ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
        'VOICE_NOT_CONFIGURED',
        500
      );
    }

    // 2. ElevenLabs API í˜¸ì¶œ
    const startTime = Date.now();

    const audioStream = await elevenlabs.generate({
      voice: voiceId || WILLIAM_VOICE_ID,
      text: text.trim(),
      model_id: modelId || 'eleven_turbo_v2', // ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸
      output_format: 'mp3_44100_128', // ì›¹ ìµœì í™”
    });

    const duration = Date.now() - startTime;

    // 3. ìŠ¤íŠ¸ë¦¼ì„ Bufferë¡œ ë³€í™˜
    const chunks: Uint8Array[] = [];
    for await (const chunk of audioStream) {
      chunks.push(chunk);
    }
    const audioBuffer = Buffer.concat(chunks);

    // 4. ì‘ë‹µ í—¤ë” ì„¤ì •
    const headers = new Headers({
      'Content-Type': 'audio/mpeg',
      'Content-Length': audioBuffer.length.toString(),
      'X-Generation-Duration': duration.toString(),
      'Cache-Control': 'public, max-age=31536000', // 1ë…„ ìºì‹±
    });

    return new Response(audioBuffer, { headers });

  } catch (error) {
    console.error('TTS Error:', error);

    if (error instanceof VoiceError) {
      return NextResponse.json(
        { error: error.message, code: error.code },
        { status: error.statusCode }
      );
    }

    return NextResponse.json(
      { error: 'ìŒì„± í•©ì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.', code: 'TTS_ERROR' },
      { status: 500 }
    );
  }
}
```

---

#### âœ… Task 1.6: í†µí•© ìŒì„± ì±„íŒ… API

**íŒŒì¼**: `app/api/voice/chat/route.ts` (ì‹ ê·œ)

```typescript
/**
 * í†µí•© ìŒì„± ì±„íŒ… API
 * STT â†’ RAG â†’ TTSë¥¼ í•˜ë‚˜ì˜ ì—”ë“œí¬ì¸íŠ¸ë¡œ í†µí•©
 */

import { NextRequest, NextResponse } from 'next/server';
import OpenAI from 'openai';
import { ElevenLabsClient } from 'elevenlabs';
import { createClient } from '@supabase/supabase-js';
import { CohereClient } from 'cohere-ai';
import { VoiceError } from '@/types/voice';
import { v4 as uuidv4 } from 'uuid';

export const runtime = 'nodejs';
export const maxDuration = 60; // 60ì´ˆ íƒ€ì„ì•„ì›ƒ

// í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const elevenlabs = new ElevenLabsClient({ apiKey: process.env.ELEVENLABS_API_KEY });
const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!
);
const cohere = process.env.COHERE_API_KEY
  ? new CohereClient({ token: process.env.COHERE_API_KEY })
  : null;

const WILLIAM_VOICE_ID = process.env.WILLIAM_VOICE_ID || '';

export async function POST(req: NextRequest) {
  const startTime = Date.now();
  const metrics = {
    sttDuration: 0,
    ragDuration: 0,
    ttsDuration: 0,
    totalDuration: 0,
  };

  try {
    // 1. ì…ë ¥ ë°›ê¸°
    const formData = await req.formData();
    const audioFile = formData.get('audio') as File;
    const sessionId = formData.get('sessionId') as string || uuidv4();
    const historyJson = formData.get('history') as string;
    const history = historyJson ? JSON.parse(historyJson) : [];

    if (!audioFile) {
      throw new VoiceError('ìŒì„± íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.', 'NO_AUDIO', 400);
    }

    // 2. STT: ìŒì„± â†’ í…ìŠ¤íŠ¸
    console.log('[Voice Chat] STT ì‹œì‘...');
    const sttStart = Date.now();

    const transcription = await openai.audio.transcriptions.create({
      file: audioFile,
      model: 'whisper-1',
      language: 'ko',
    });

    const userQuery = transcription.text;
    metrics.sttDuration = Date.now() - sttStart;
    console.log(`[Voice Chat] STT ì™„ë£Œ: "${userQuery}" (${metrics.sttDuration}ms)`);

    // 3. RAG: ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ & ë‹µë³€ ìƒì„±
    console.log('[Voice Chat] RAG ì‹œì‘...');
    const ragStart = Date.now();

    // 3.1 ì„ë² ë”© ìƒì„±
    const embeddingResponse = await openai.embeddings.create({
      model: 'text-embedding-3-small',
      input: userQuery,
    });
    const queryEmbedding = embeddingResponse.data[0].embedding;

    // 3.2 ë²¡í„° ê²€ìƒ‰
    const { data: documents } = await supabase.rpc('match_documents', {
      query_embedding: queryEmbedding,
      match_threshold: 0.2,
      match_count: 20,
    });

    // 3.3 Reranking (ì„ íƒ)
    let rerankedDocuments = documents || [];
    if (cohere && documents && documents.length > 1) {
      try {
        const rerankedResults = await cohere.rerank({
          query: userQuery,
          documents: documents.map((doc: any) => doc.content),
          topN: Math.min(5, documents.length),
          model: 'rerank-english-v3.0',
        });
        rerankedDocuments = rerankedResults.results.map(
          (result: any) => documents[result.index]
        );
      } catch {
        rerankedDocuments = documents.slice(0, 5);
      }
    } else if (documents) {
      rerankedDocuments = documents.slice(0, 5);
    }

    // 3.4 ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    const context = rerankedDocuments
      .map((doc: any, i: number) => `[ì¶œì²˜ ${i + 1}] ${doc.content}`)
      .join('\n\n');

    // 3.5 ìŒì„±ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì§§ê²Œ ìµœì í™”)
    const systemPrompt = `ë‹¹ì‹ ì€ William Jungì…ë‹ˆë‹¤.

ìŒì„± ëŒ€í™”ì´ë¯€ë¡œ:
- ê°„ê²°í•˜ê²Œ ë‹µë³€ (2-3ë¬¸ì¥, ìµœëŒ€ 150ë‹¨ì–´)
- ìì—°ìŠ¤ëŸ¬ìš´ ë§íˆ¬ ("~ê±°ì•¼", "~ì§€", "ê·¼ë°")
- ì¶œì²˜ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰ ("ë‚´ê°€ ì“´ ê¸€ì—ì„œ..."ê°€ ì•„ë‹ˆë¼ ì§ì ‘ ë‹µë³€)

${context}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì¹œêµ¬ì—ê²Œ ë§í•˜ë“¯ ë‹µë³€í•˜ì„¸ìš”.`;

    // 3.6 LLM ë‹µë³€ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° ì•„ë‹˜, Phase 1)
    const completion = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [
        { role: 'system', content: systemPrompt },
        ...history.slice(-6), // ìµœê·¼ 3í„´ë§Œ
        { role: 'user', content: userQuery },
      ],
      temperature: 0.6,
      max_tokens: 200, // ìŒì„±ì´ë¯€ë¡œ ì§§ê²Œ
    });

    const responseText = completion.choices[0].message.content ||
      'ì£„ì†¡í•´ìš”, ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ì—ˆì–´ìš”.';

    metrics.ragDuration = Date.now() - ragStart;
    console.log(`[Voice Chat] RAG ì™„ë£Œ: ${metrics.ragDuration}ms`);

    // 4. TTS: í…ìŠ¤íŠ¸ â†’ ìŒì„±
    console.log('[Voice Chat] TTS ì‹œì‘...');
    const ttsStart = Date.now();

    const audioStream = await elevenlabs.generate({
      voice: WILLIAM_VOICE_ID,
      text: responseText,
      model_id: 'eleven_turbo_v2',
      output_format: 'mp3_44100_128',
    });

    // ìŠ¤íŠ¸ë¦¼ì„ Bufferë¡œ ë³€í™˜
    const chunks: Uint8Array[] = [];
    for await (const chunk of audioStream) {
      chunks.push(chunk);
    }
    const audioBuffer = Buffer.concat(chunks);

    metrics.ttsDuration = Date.now() - ttsStart;
    console.log(`[Voice Chat] TTS ì™„ë£Œ: ${metrics.ttsDuration}ms`);

    // 5. ì‘ë‹µ êµ¬ì„±
    metrics.totalDuration = Date.now() - startTime;

    // ë©”íƒ€ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ, ì˜¤ë””ì˜¤ë¥¼ Base64ë¡œ ë°˜í™˜
    const response = {
      sessionId,
      transcript: userQuery,
      response: responseText,
      audioBase64: audioBuffer.toString('base64'),
      sources: rerankedDocuments.map((doc: any) => ({
        title: doc.title || 'ì œëª© ì—†ìŒ',
        content: doc.content.substring(0, 200),
      })),
      metrics,
    };

    console.log(`[Voice Chat] ì´ ì†Œìš” ì‹œê°„: ${metrics.totalDuration}ms`);
    console.log(`  - STT: ${metrics.sttDuration}ms`);
    console.log(`  - RAG: ${metrics.ragDuration}ms`);
    console.log(`  - TTS: ${metrics.ttsDuration}ms`);

    return NextResponse.json(response);

  } catch (error) {
    console.error('[Voice Chat] Error:', error);

    if (error instanceof VoiceError) {
      return NextResponse.json(
        { error: error.message, code: error.code },
        { status: error.statusCode }
      );
    }

    return NextResponse.json(
      { error: 'ìŒì„± ì±„íŒ… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.', code: 'VOICE_CHAT_ERROR' },
      { status: 500 }
    );
  }
}
```

---

### Week 1, Day 5-7: í”„ë¡ íŠ¸ì—”ë“œ êµ¬í˜„

#### âœ… Task 1.7: ìŒì„± ë…¹ìŒ ì»´í¬ë„ŒíŠ¸

**íŒŒì¼**: `components/VoiceRecorder.tsx` (ì‹ ê·œ)

```typescript
'use client';

import { useState, useRef, useCallback } from 'react';

interface VoiceRecorderProps {
  onRecordingComplete: (audioBlob: Blob) => void;
  maxDuration?: number; // ì´ˆ ë‹¨ìœ„
}

export default function VoiceRecorder({
  onRecordingComplete,
  maxDuration = 60,
}: VoiceRecorderProps) {
  const [isRecording, setIsRecording] = useState(false);
  const [recordingTime, setRecordingTime] = useState(0);
  const [isPaused, setIsPaused] = useState(false);

  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const chunksRef = useRef<Blob[]>([]);
  const timerRef = useRef<NodeJS.Timeout | null>(null);
  const streamRef = useRef<MediaStream | null>(null);

  // ë…¹ìŒ ì‹œì‘
  const startRecording = useCallback(async () => {
    try {
      // ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          sampleRate: 44100,
        },
      });

      streamRef.current = stream;

      // MediaRecorder ì„¤ì •
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus',
      });

      mediaRecorderRef.current = mediaRecorder;
      chunksRef.current = [];

      // ë°ì´í„° ìˆ˜ì§‘
      mediaRecorder.ondataavailable = (e) => {
        if (e.data.size > 0) {
          chunksRef.current.push(e.data);
        }
      };

      // ë…¹ìŒ ì¢…ë£Œ ì‹œ
      mediaRecorder.onstop = () => {
        const audioBlob = new Blob(chunksRef.current, { type: 'audio/webm' });
        onRecordingComplete(audioBlob);

        // ì •ë¦¬
        if (streamRef.current) {
          streamRef.current.getTracks().forEach(track => track.stop());
        }
        setRecordingTime(0);
      };

      // ë…¹ìŒ ì‹œì‘
      mediaRecorder.start(1000); // 1ì´ˆë§ˆë‹¤ ë°ì´í„° ìˆ˜ì§‘
      setIsRecording(true);

      // íƒ€ì´ë¨¸ ì‹œì‘
      timerRef.current = setInterval(() => {
        setRecordingTime(prev => {
          if (prev + 1 >= maxDuration) {
            stopRecording();
            return maxDuration;
          }
          return prev + 1;
        });
      }, 1000);

    } catch (error) {
      console.error('ë§ˆì´í¬ ì ‘ê·¼ ì‹¤íŒ¨:', error);
      alert('ë§ˆì´í¬ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¶Œí•œì„ í™•ì¸í•´ì£¼ì„¸ìš”.');
    }
  }, [maxDuration, onRecordingComplete]);

  // ë…¹ìŒ ì¤‘ì§€
  const stopRecording = useCallback(() => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();
      setIsRecording(false);

      if (timerRef.current) {
        clearInterval(timerRef.current);
      }
    }
  }, [isRecording]);

  // ì¼ì‹œì •ì§€/ì¬ê°œ
  const togglePause = useCallback(() => {
    if (mediaRecorderRef.current) {
      if (isPaused) {
        mediaRecorderRef.current.resume();
      } else {
        mediaRecorderRef.current.pause();
      }
      setIsPaused(!isPaused);
    }
  }, [isPaused]);

  // í¬ë§·íŒ…: ì´ˆ â†’ MM:SS
  const formatTime = (seconds: number) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
  };

  return (
    <div className="flex flex-col items-center gap-4 p-6 bg-white rounded-lg shadow-lg">
      {/* íƒ€ì´ë¨¸ */}
      <div className="text-4xl font-mono text-gray-800">
        {formatTime(recordingTime)}
        <span className="text-sm text-gray-500 ml-2">
          / {formatTime(maxDuration)}
        </span>
      </div>

      {/* ì‹œê°í™” (ì„ íƒ) */}
      {isRecording && (
        <div className="flex gap-1 h-12 items-end">
          {[...Array(20)].map((_, i) => (
            <div
              key={i}
              className="w-1 bg-blue-500 rounded-t animate-pulse"
              style={{
                height: `${Math.random() * 100}%`,
                animationDelay: `${i * 50}ms`,
              }}
            />
          ))}
        </div>
      )}

      {/* ì»¨íŠ¸ë¡¤ ë²„íŠ¼ */}
      <div className="flex gap-4">
        {!isRecording ? (
          <button
            onClick={startRecording}
            className="px-8 py-4 bg-blue-600 text-white rounded-full hover:bg-blue-700
                     flex items-center gap-2 text-lg font-semibold transition-all
                     shadow-lg hover:shadow-xl"
          >
            ğŸ¤ ë…¹ìŒ ì‹œì‘
          </button>
        ) : (
          <>
            <button
              onClick={togglePause}
              className="px-6 py-3 bg-yellow-500 text-white rounded-full hover:bg-yellow-600"
            >
              {isPaused ? 'â–¶ï¸ ì¬ê°œ' : 'â¸ï¸ ì¼ì‹œì •ì§€'}
            </button>
            <button
              onClick={stopRecording}
              className="px-6 py-3 bg-red-600 text-white rounded-full hover:bg-red-700"
            >
              â¹ï¸ ì™„ë£Œ
            </button>
          </>
        )}
      </div>

      {/* ì•ˆë‚´ í…ìŠ¤íŠ¸ */}
      <p className="text-sm text-gray-600 text-center">
        {!isRecording
          ? 'Williamì—ê²Œ í•˜ê³  ì‹¶ì€ ë§ì„ ë…¹ìŒí•˜ì„¸ìš”'
          : isPaused
          ? 'ì¼ì‹œì •ì§€ë¨'
          : 'ë…¹ìŒ ì¤‘... ì™„ë£Œ ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ Williamì´ ë‹µë³€í•©ë‹ˆë‹¤'}
      </p>
    </div>
  );
}
```

---

#### âœ… Task 1.8: ìŒì„± ì±„íŒ… ì¸í„°í˜ì´ìŠ¤

**íŒŒì¼**: `components/VoiceChatInterface.tsx` (ì‹ ê·œ)

```typescript
'use client';

import { useState, useRef } from 'react';
import VoiceRecorder from './VoiceRecorder';

interface Message {
  role: 'user' | 'assistant';
  content: string;
  audioUrl?: string;
  timestamp: Date;
}

export default function VoiceChatInterface() {
  const [messages, setMessages] = useState<Message[]>([]);
  const [isProcessing, setIsProcessing] = useState(false);
  const [sessionId] = useState(() => crypto.randomUUID());
  const audioRef = useRef<HTMLAudioElement>(null);

  // ë…¹ìŒ ì™„ë£Œ ì‹œ ì²˜ë¦¬
  const handleRecordingComplete = async (audioBlob: Blob) => {
    setIsProcessing(true);

    try {
      // FormData êµ¬ì„±
      const formData = new FormData();
      formData.append('audio', audioBlob, 'recording.webm');
      formData.append('sessionId', sessionId);
      formData.append('history', JSON.stringify(
        messages.map(m => ({ role: m.role, content: m.content }))
      ));

      // API í˜¸ì¶œ
      const response = await fetch('/api/voice/chat', {
        method: 'POST',
        body: formData,
      });

      if (!response.ok) {
        throw new Error('ìŒì„± ì±„íŒ… ì‹¤íŒ¨');
      }

      const data = await response.json();

      // ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
      const userMessage: Message = {
        role: 'user',
        content: data.transcript,
        timestamp: new Date(),
      };

      // AI ì‘ë‹µ ì¶”ê°€
      const assistantMessage: Message = {
        role: 'assistant',
        content: data.response,
        audioUrl: `data:audio/mpeg;base64,${data.audioBase64}`,
        timestamp: new Date(),
      };

      setMessages(prev => [...prev, userMessage, assistantMessage]);

      // ìë™ ì¬ìƒ
      if (audioRef.current) {
        audioRef.current.src = assistantMessage.audioUrl!;
        audioRef.current.play();
      }

    } catch (error) {
      console.error('ìŒì„± ì±„íŒ… ì˜¤ë¥˜:', error);
      alert('ìŒì„± ì±„íŒ… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
    } finally {
      setIsProcessing(false);
    }
  };

  // ìŒì„± ì¬ìƒ
  const playAudio = (audioUrl: string) => {
    if (audioRef.current) {
      audioRef.current.src = audioUrl;
      audioRef.current.play();
    }
  };

  return (
    <div className="max-w-4xl mx-auto p-6">
      <h1 className="text-3xl font-bold mb-8 text-center">
        ğŸ™ï¸ Williamê³¼ ìŒì„± ëŒ€í™”
      </h1>

      {/* ëŒ€í™” ë‚´ì—­ */}
      <div className="mb-8 space-y-4 max-h-96 overflow-y-auto">
        {messages.length === 0 ? (
          <div className="text-center text-gray-500 py-12">
            ì•„ì§ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ ì‹œì‘í•˜ì„¸ìš”!
          </div>
        ) : (
          messages.map((msg, idx) => (
            <div
              key={idx}
              className={`flex ${msg.role === 'user' ? 'justify-end' : 'justify-start'}`}
            >
              <div
                className={`max-w-lg p-4 rounded-lg ${
                  msg.role === 'user'
                    ? 'bg-blue-500 text-white'
                    : 'bg-gray-200 text-gray-800'
                }`}
              >
                <div className="flex items-start gap-2">
                  <span className="text-2xl">
                    {msg.role === 'user' ? 'ğŸ‘¤' : 'ğŸ¤–'}
                  </span>
                  <div className="flex-1">
                    <p className="text-sm font-semibold mb-1">
                      {msg.role === 'user' ? 'You' : 'William'}
                    </p>
                    <p className="whitespace-pre-wrap">{msg.content}</p>

                    {/* William ì‘ë‹µì—ë§Œ ì¬ìƒ ë²„íŠ¼ */}
                    {msg.role === 'assistant' && msg.audioUrl && (
                      <button
                        onClick={() => playAudio(msg.audioUrl!)}
                        className="mt-2 text-xs underline hover:text-blue-600"
                      >
                        ğŸ”Š ë‹¤ì‹œ ë“£ê¸°
                      </button>
                    )}
                  </div>
                </div>

                <div className="text-xs opacity-70 mt-2">
                  {msg.timestamp.toLocaleTimeString('ko-KR')}
                </div>
              </div>
            </div>
          ))
        )}
      </div>

      {/* ë…¹ìŒ ì»´í¬ë„ŒíŠ¸ */}
      {isProcessing ? (
        <div className="text-center py-12">
          <div className="inline-block animate-spin rounded-full h-12 w-12 border-b-2 border-blue-600"></div>
          <p className="mt-4 text-gray-600">Williamì´ ìƒê°í•˜ëŠ” ì¤‘...</p>
        </div>
      ) : (
        <VoiceRecorder
          onRecordingComplete={handleRecordingComplete}
          maxDuration={60}
        />
      )}

      {/* ìˆ¨ê²¨ì§„ ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´ */}
      <audio ref={audioRef} className="hidden" />
    </div>
  );
}
```

---

#### âœ… Task 1.9: ìŒì„± ì±„íŒ… í˜ì´ì§€

**íŒŒì¼**: `app/voice/page.tsx` (ì‹ ê·œ)

```typescript
import VoiceChatInterface from '@/components/VoiceChatInterface';

export const metadata = {
  title: 'Williamê³¼ ìŒì„± ëŒ€í™” | My Bttrfly',
  description: 'William Jungê³¼ ìŒì„±ìœ¼ë¡œ ëŒ€í™”í•˜ì„¸ìš”',
};

export default function VoicePage() {
  return (
    <main className="min-h-screen bg-gradient-to-b from-blue-50 to-white py-12">
      <VoiceChatInterface />
    </main>
  );
}
```

---

## ğŸ“Š Phase 1 ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

```
âœ… í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (.env.local)
âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ (elevenlabs, formidable)
âœ… íƒ€ì… ì •ì˜ (types/voice.ts)
âœ… STT API (/api/voice/transcribe)
âœ… TTS API (/api/voice/synthesize)
âœ… í†µí•© API (/api/voice/chat)
âœ… ë…¹ìŒ ì»´í¬ë„ŒíŠ¸ (VoiceRecorder.tsx)
âœ… ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ (VoiceChatInterface.tsx)
âœ… í˜ì´ì§€ (app/voice/page.tsx)
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ê³„íš

### Unit Tests

**íŒŒì¼**: `__tests__/voice/api.test.ts`

```typescript
import { describe, it, expect } from '@jest/globals';

describe('Voice API Tests', () => {
  describe('STT API', () => {
    it('should transcribe Korean audio', async () => {
      // TODO: ìƒ˜í”Œ ì˜¤ë””ì˜¤ë¡œ í…ŒìŠ¤íŠ¸
    });

    it('should reject files over 25MB', async () => {
      // TODO
    });
  });

  describe('TTS API', () => {
    it('should generate audio from text', async () => {
      // TODO
    });

    it('should reject empty text', async () => {
      // TODO
    });
  });

  describe('Voice Chat API', () => {
    it('should handle full conversation flow', async () => {
      // TODO
    });
  });
});
```

### Manual Tests

**ì²´í¬ë¦¬ìŠ¤íŠ¸**:

```
â–¡ ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­ ì‘ë™
â–¡ ë…¹ìŒ ì‹œì‘/ì¤‘ì§€ ì‘ë™
â–¡ 60ì´ˆ ì œí•œ ì‘ë™
â–¡ STT ì •í™•ë„ (í•œêµ­ì–´)
  - ì¼ë°˜ ëŒ€í™”: "ì•ˆë…•í•˜ì„¸ìš” RAGê°€ ë­ì˜ˆìš”?"
  - ì „ë¬¸ ìš©ì–´: "Retrieval Augmented Generation"
  - ë¹ ë¥¸ ë§
  - ëŠë¦° ë§
  - ë°°ê²½ ì†ŒìŒ ìˆì„ ë•Œ

â–¡ RAG í’ˆì§ˆ
  - ê´€ë ¨ ë¬¸ì„œ ì˜ ì°¾ëŠ”ì§€
  - ë‹µë³€ì´ ìì—°ìŠ¤ëŸ¬ìš´ì§€
  - ì¶œì²˜ ì •í™•í•œì§€

â–¡ TTS í’ˆì§ˆ
  - William ëª©ì†Œë¦¬ì™€ ìœ ì‚¬í•œì§€
  - ë°œìŒ ì •í™•í•œì§€
  - ìì—°ìŠ¤ëŸ¬ìš´ ì–µì–‘
  - ì‰¼í‘œ, ê°•ì¡° ì ì ˆí•œì§€

â–¡ ì „ì²´ ì§€ì—°ì‹œê°„
  - ëª©í‘œ: 5ì´ˆ ì´ë‚´
  - STT: 1ì´ˆ ì´ë‚´
  - RAG: 2ì´ˆ ì´ë‚´
  - TTS: 2ì´ˆ ì´ë‚´

â–¡ ì—ëŸ¬ ì²˜ë¦¬
  - ë§ˆì´í¬ ì—†ì„ ë•Œ
  - ë„¤íŠ¸ì›Œí¬ ëŠê¸¸ ë•Œ
  - API ì˜¤ë¥˜ ì‹œ
```

---

## ğŸš€ ë°°í¬ ê³„íš

### Vercel ë°°í¬

```bash
# 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
vercel env add ELEVENLABS_API_KEY
vercel env add WILLIAM_VOICE_ID

# 2. ë°°í¬
vercel --prod

# 3. í•¨ìˆ˜ ì„¤ì • í™•ì¸
# vercel.json
{
  "functions": {
    "app/api/voice/chat/route.ts": {
      "maxDuration": 60
    }
  }
}
```

---

## ğŸ“ˆ ì„±ê³µ ì§€í‘œ

### Phase 1 ëª©í‘œ

| ì§€í‘œ | ëª©í‘œ | ì¸¡ì • ë°©ë²• |
|-----|------|----------|
| **ì™„ë£Œìœ¨** | 80%+ | ë…¹ìŒ ì‹œì‘ â†’ ì‘ë‹µ ì™„ë£Œ |
| **ë§Œì¡±ë„** | 4/5+ | 20ëª… ì„¤ë¬¸ |
| **ì§€ì—°ì‹œê°„** | < 5ì´ˆ | ì„œë²„ ë¡œê·¸ |
| **STT ì •í™•ë„** | 90%+ | ìˆ˜ë™ ê²€ì¦ |
| **TTS ìì—°ìŠ¤ëŸ¬ì›€** | 4/5+ | ì£¼ê´€ì  í‰ê°€ |
| **ì¬ì‚¬ìš© ì˜í–¥** | 60%+ | "ë˜ ì“°ê³  ì‹¶ë‹¤" |

### ë°ì´í„° ìˆ˜ì§‘

**íŒŒì¼**: `lib/analytics.ts` (ì‹ ê·œ)

```typescript
export function trackVoiceMetrics(data: {
  sessionId: string;
  sttDuration: number;
  ragDuration: number;
  ttsDuration: number;
  totalDuration: number;
  transcriptLength: number;
  responseLength: number;
}) {
  // Vercel Analytics ë˜ëŠ” Google Analytics
  console.log('[Analytics]', data);

  // TODO: ì‹¤ì œ ë¶„ì„ ë„êµ¬ ì—°ë™
}
```

---

## ğŸ”„ Phase 2: ìŠ¤íŠ¸ë¦¬ë° TTS (ì„ íƒ)

Phase 1 í…ŒìŠ¤íŠ¸ í›„ ì‚¬ìš©ì í”¼ë“œë°±ì´ ì¢‹ìœ¼ë©´ ì§„í–‰.

### ëª©í‘œ
- ì²« ë¬¸ì¥ ì¬ìƒ: 1ì´ˆ ì´ë‚´
- ì²´ê° ì§€ì—°ì‹œê°„: ì‹¤ì‹œê°„ì²˜ëŸ¼

### êµ¬í˜„
- Streaming TTS (ë¬¸ì¥ë³„)
- ì ì§„ì  ì˜¤ë””ì˜¤ ì¬ìƒ
- ë²„í¼ ê´€ë¦¬

**ìƒì„¸ ê³„íšì€ Phase 1 ì™„ë£Œ í›„ ì‘ì„±**

---

## ğŸ™ï¸ William ëª©ì†Œë¦¬ í´ë¡œë‹ ê°€ì´ë“œ

### ì¤€ë¹„ë¬¼
- ğŸ¤ ì¢‹ì€ ë§ˆì´í¬ (ë˜ëŠ” ì¡°ìš©í•œ í™˜ê²½ì˜ ìŠ¤ë§ˆíŠ¸í°)
- â±ï¸ ì‹œê°„ 10ë¶„
- ğŸ“ ìŠ¤í¬ë¦½íŠ¸

### ë…¹ìŒ ê°€ì´ë“œ

**ElevenLabs ë¬´ë£Œ í‹°ì–´**: 1ë¶„ ìƒ˜í”Œ
**Professional ($99)**: 3-5ë¶„ ìƒ˜í”Œ (ë” ë‚˜ì€ í’ˆì§ˆ)

**ë…¹ìŒ ìŠ¤í¬ë¦½íŠ¸** (ë‹¤ì–‘í•œ í†¤ìœ¼ë¡œ):

```
1. ì¤‘ë¦½ì  (30ì´ˆ):
"ì•ˆë…•í•˜ì„¸ìš”. Williamì…ë‹ˆë‹¤. ì˜¤ëŠ˜ì€ AIì™€ ì°½ì—…ì— ëŒ€í•´
ì´ì•¼ê¸°í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ì œê°€ ìµœê·¼ì— ê²½í—˜í•œ ê²ƒë“¤ì„
ê³µìœ í•˜ê³  ì‹¶ì–´ìš”."

2. í¥ë¶„ë¨ (30ì´ˆ):
"ì™€, ì´ê±° ì •ë§ ë©‹ì§„ë°ìš”! ë°©ê¸ˆ ë°œê²¬í•œ ì´ ê¸°ìˆ ì´
ê²Œì„ ì²´ì¸ì €ê°€ ë  ê²ƒ ê°™ì•„ìš”. ì—¬ëŸ¬ë¶„ë„ ê¼­ ì‹œë„í•´ë³´ì„¸ìš”!"

3. ì‚¬ìƒ‰ì  (30ì´ˆ):
"ìƒê°í•´ë³´ë©´, ìš°ë¦¬ê°€ í•˜ëŠ” ì¼ì˜ ë³¸ì§ˆì€ ë­˜ê¹Œìš”?
ë‹¨ìˆœíˆ ê¸°ìˆ ì„ ë§Œë“œëŠ” ê²Œ ì•„ë‹ˆë¼, ì‚¬ëŒë“¤ì˜ ë¬¸ì œë¥¼
í•´ê²°í•˜ëŠ” ê±°ë¼ê³  ìƒê°í•´ìš”."

4. ì„¤ëª… (30ì´ˆ):
"RAGëŠ” Retrieval Augmented Generationì˜ ì•½ìì¸ë°,
ì‰½ê²Œ ë§í•˜ë©´ AIê°€ í•„ìš”í•œ ì •ë³´ë¥¼ ì°¾ì•„ì„œ ë‹µë³€ì—
í™œìš©í•˜ëŠ” ë°©ì‹ì´ì—ìš”. ê½¤ íš¨ê³¼ì ì´ì£ ."
```

**ë…¹ìŒ íŒ**:
- ì¡°ìš©í•œ í™˜ê²½ (ì—ì–´ì»¨, íŒ¬ ë„ê¸°)
- ë§ˆì´í¬ì™€ 20-30cm ê±°ë¦¬
- ìì—°ìŠ¤ëŸ½ê²Œ ë§í•˜ê¸° (ëŒ€ë³¸ ì½ëŠ” ëŠë‚Œ X)
- ìˆ¨ì†Œë¦¬, ì…ìˆ  ì†Œë¦¬ ìµœì†Œí™”
- ê° í†¤ì„ ëª…í™•í•˜ê²Œ êµ¬ë¶„

**ì—…ë¡œë“œ**:
1. ElevenLabs ê°€ì…
2. Voice Library â†’ Add Voice
3. íŒŒì¼ ì—…ë¡œë“œ
4. Voice ID ë°›ê¸° (`process.env.WILLIAM_VOICE_ID`)

---

## ğŸ’° ë¹„ìš© ì‹œë®¬ë ˆì´ì…˜

### ì‹œë‚˜ë¦¬ì˜¤ 1: ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸

**ì‚¬ìš©ëŸ‰**: ì›” 50 ëŒ€í™” Ã— 5ë¶„ = 250ë¶„

```
STT: 250ë¶„ Ã— $0.003 = $0.75
LLM: ê¸°ì¡´ ì‚¬ìš© (ì¶”ê°€ ê±°ì˜ ì—†ìŒ)
TTS: 250ë¶„ Ã— $0.03 = $7.50

ì´: $8.25/ì›” (ì•½ 1.1ë§Œì›)
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: ì¤‘ê°„ ê·œëª¨

**ì‚¬ìš©ëŸ‰**: ì›” 500 ëŒ€í™” Ã— 7ë¶„ = 3,500ë¶„

```
STT: 3,500 Ã— $0.003 = $10.50
LLM: ~$5
TTS: 3,500 Ã— $0.03 = $105

ì´: $120.50/ì›” (ì•½ 16ë§Œì›)
```

### ì‹œë‚˜ë¦¬ì˜¤ 3: ëŒ€ê·œëª¨

**ì‚¬ìš©ëŸ‰**: ì›” 5,000 ëŒ€í™” Ã— 10ë¶„ = 50,000ë¶„

```
STT: 50,000 Ã— $0.003 = $150
LLM: ~$50
TTS: 50,000 Ã— $0.03 = $1,500

ì´: $1,700/ì›” (ì•½ 227ë§Œì›)
ëŒ€í™”ë‹¹: $0.34 (450ì›)

ì´ ê·œëª¨ì—ì„œëŠ”:
- ìì²´ í˜¸ìŠ¤íŒ… ê³ ë ¤ (GPU $500/ì›”)
- ë˜ëŠ” ElevenLabs ëŒ€ëŸ‰ í• ì¸ í˜‘ìƒ
```

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„

### ì§€ê¸ˆ ë°”ë¡œ (30ë¶„)
1. ElevenLabs ê°€ì…
2. William ëª©ì†Œë¦¬ 1ë¶„ ë…¹ìŒ
3. Voice ID ë°œê¸‰

### ë‚´ì¼ (Day 1)
1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
2. íŒ¨í‚¤ì§€ ì„¤ì¹˜
3. íƒ€ì… ì •ì˜ ì‘ì„±

### ì´ë²ˆ ì£¼ (Day 2-7)
1. ë°±ì—”ë“œ API 3ê°œ êµ¬í˜„
2. í”„ë¡ íŠ¸ì—”ë“œ ì»´í¬ë„ŒíŠ¸ 3ê°œ êµ¬í˜„
3. í†µí•© í…ŒìŠ¤íŠ¸

### ë‹¤ìŒ ì£¼
1. 20ëª… ë² íƒ€ í…ŒìŠ¤íŠ¸
2. í”¼ë“œë°± ìˆ˜ì§‘
3. Decision: Phase 2 ì§„í–‰ ì—¬ë¶€

---

## âš ï¸ í”¼í•´ì•¼ í•  ì‹¤ìˆ˜ (ì‚¬ë¡€ ê¸°ë°˜)

### 1. OpenAI Realtime API ë„ˆë¬´ ë¹¨ë¦¬ ì‚¬ìš©
âŒ **í•˜ì§€ ë§ˆì„¸ìš”**: "ê°€ì¥ ë¹ ë¥¸ ê±°ë‹ˆê¹Œ ì²˜ìŒë¶€í„° ì“°ì"
âœ… **ëŒ€ì‹ **: Phase 1-2ë¡œ ê²€ì¦ â†’ ì •ë§ í•„ìš”í•˜ë©´ ê·¸ë•Œ

**ì´ìœ **:
- ë¹„ìš© 10ë°° ($0.30 vs $0.03)
- Phase 2 ìŠ¤íŠ¸ë¦¬ë°ë„ ì¶©ë¶„íˆ ë¹ ë¦„ (1ì´ˆ)
- ëŒ€ë¶€ë¶„ ì‚¬ìš©ìëŠ” ì°¨ì´ ëª» ëŠë‚Œ

### 2. ì¤‘ë‹¨ ê¸°ëŠ¥ ê³¼ë„í•˜ê²Œ ë¯¼ê°í•˜ê²Œ
âŒ **í•˜ì§€ ë§ˆì„¸ìš”**: "ì‚¬ìš©ìê°€ ìˆ¨ì‰¬ë©´ ì¤‘ë‹¨"
âœ… **ëŒ€ì‹ **: Phase 1ì—ì„œëŠ” ì¤‘ë‹¨ ì—†ì´ ì‹œì‘

**ì´ìœ **:
- ChatGPTì˜ ê°€ì¥ í° ë¶ˆë§Œ
- ìì—°ìŠ¤ëŸ¬ìš´ ì‰¼í‘œë¥¼ ëìœ¼ë¡œ ì˜¤ì¸
- í„´ ê¸°ë°˜ìœ¼ë¡œë„ ì¶©ë¶„íˆ ìì—°ìŠ¤ëŸ¬ì›€

### 3. ì™„ë²½í•œ ìŒì„± í’ˆì§ˆ ì§‘ì°©
âŒ **í•˜ì§€ ë§ˆì„¸ìš”**: Professional Voice ($99) ë¨¼ì €
âœ… **ëŒ€ì‹ **: ë¬´ë£Œ í´ë¡œë‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ â†’ í”¼ë“œë°± í›„ ì—…ê·¸ë ˆì´ë“œ

**ì´ìœ **:
- 80% í’ˆì§ˆë¡œë„ ì¶©ë¶„íˆ ì¸ìƒì 
- ì‚¬ìš©ìë“¤ì€ ì½˜í…ì¸  í’ˆì§ˆì„ ë” ì¤‘ìš”í•˜ê²Œ ë´„

### 4. ëª¨ë“  ì–¸ì–´ ì§€ì›í•˜ë ¤ê³ 
âŒ **í•˜ì§€ ë§ˆì„¸ìš”**: "í•œì˜ì¤‘ì¼ ë‹¤ ì§€ì›!"
âœ… **ëŒ€ì‹ **: í•œêµ­ì–´ë§Œ ì™„ë²½í•˜ê²Œ

**ì´ìœ **:
- William ì½˜í…ì¸ ê°€ í•œêµ­ì–´ ì¤‘ì‹¬
- ë‹¤êµ­ì–´ëŠ” ë³µì¡ë„ 3ë°°, ë¹„ìš© 2ë°°
- ì‚¬ìš©ì í”¼ë“œë°± ë³´ê³  ì¶”ê°€

---

## ğŸ¯ ìµœì¢… ìš”ì•½

### âœ… ì¶”ì²œ (ROI ìµœê³ )

**Phase 1 (ë¹„ë™ê¸° ìŒì„±)**:
- ë¹„ìš©: ì›” $33 (100 ëŒ€í™”)
- ê°œë°œ: 1ì£¼
- íš¨ê³¼: â­â­â­â­

**Phase 2 (ìŠ¤íŠ¸ë¦¬ë°)**:
- ë¹„ìš©: ë™ì¼
- ê°œë°œ: +1ì£¼
- íš¨ê³¼: â­â­â­â­â­

**ì´ íˆ¬ì**: 2ì£¼, ì›” 4ë§Œì›
**ROI**: ìµœê³  (ë°”ì´ëŸ´ ê°€ëŠ¥ì„± + ì‹¤ì‚¬ìš©)

### ğŸ“ˆ ì„±ê³µ ì§€í‘œ

- Phase 1 í›„: 20ëª… ì¤‘ 15ëª…+ "ê³„ì† ì“°ê³  ì‹¶ë‹¤"
- Phase 2 í›„: ëŒ€í™” ì™„ë£Œìœ¨ 80%+
- í‰ê·  ëŒ€í™” ê¸¸ì´ 5ë¶„+
